<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG" />
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta
      property="og:description"
      content="SOCIAL MEDIA DESCRIPTION TAG TAG"
    />
    <meta property="og:url" content="URL OF THE WEBSITE" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG" />
    <meta
      name="twitter:description"
      content="TWITTER BANNER DESCRIPTION META TAG"
    />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta
      name="twitter:image"
      content="static/images/your_twitter_banner_image.png"
    />
    <meta name="twitter:card" content="summary_large_image" />
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>R3ST</title>
    <link
      rel="icon"
      href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸš—</text></svg>"
    />

    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="static/css/index.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
  </head>

  <body class="wide">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                R3ST: A Synthetic Dataset with Real Trajectories for Urban
                Traffic Analysis
              </h1>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://simoneteglia.vercel.app/" target="_blank"
                    >Simone Teglia</a
                  ><sup>â—¼ï¸Ž</sup>,</span
                >
                <span class="author-block">
                  <a href="" target="_blank">Claudia Melis Tonti</a
                  ><sup>â—¼ï¸Ž</sup>,</span
                >
                <span class="author-block">
                  <a href="https://fra-pro.github.io/" target="_blank"
                    >Francesco Pro</a
                  ><sup>â—¼ï¸Ž</sup>,
                </span>

                <span class="author-block">
                  <a href="" target="_blank">Leonardo Russo</a><sup>â—¼ï¸Ž</sup>,
                </span>

                <span class="author-block">
                  <a href="" target="_blank">Andrea Alfarano</a><sup>â–²</sup>,
                </span>

                <span class="author-block">
                  <a href="" target="_blank">Matteo Pentassuglia</a
                  ><sup>â˜…</sup>,
                </span>

                <span class="author-block">
                  <a
                    href="https://sites.google.com/diag.uniroma1.it/ireneamerini"
                    target="_blank"
                    >Irene Amerini</a
                  ><sup>â—¼ï¸Ž</sup>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"
                  ><sup>â—¼ï¸Ž</sup> Sapienza University of Rome,
                  <sup>â–²</sup> INSAIT, Sofia University, <sup>â˜…</sup>EURECOM,
                  Biot, France
                  <br />
                  SynData4CV@CVPR2025 Workshops</span
                >
                <!-- <span class="eql-cntrb"
                  ><small
                    ><br /><sup>*</sup>Indicates Equal Contribution</small
                  ></span
                > -->
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Arxiv PDF link -->
                  <span class="link-block">
                    <a
                      href="https://openreview.net/forum?id=VCORXe6I5B"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Supplementary PDF link -->
                  <!-- <span class="link-block">
                    <a
                      href="static/pdfs/supplementary_material.pdf"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a
                      href="https://github.com/ALCOR-Lab-DIAG/R3ST/tree/main"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- ArXiv abstract Link -->
                  <!-- <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/<ARXIV PAPER ID>"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Teaser video-->
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            <!-- Your video here -->
            <source src="static/videos/r3st_video.mp4" type="video/mp4" />
          </video>
          <h2 class="subtitle has-text-centered"></h2>
        </div>
      </div>
    </section>
    <!-- End teaser video -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Datasets are essential to train and evaluate computer vision
                models used for traffic analysis and to enhance road safety.
                Existing real datasets fit real-world scenarios, capturing
                authentic road object behaviors, however, they typically lack
                precise ground-truth annotations. In contrast, synthetic
                datasets play a crucial role, allowing for the annotation of a
                large number of frames without additional costs or extra time.
                However, a general drawback of synthetic datasets is the lack of
                realistic vehicle motion, since trajectories are generated using
                AI models or rule-based systems. In this work, we introduce
                <b>R3ST</b> (Realistic 3D Synthetic Trajectories), a synthetic
                dataset that overcomes this limitation by generating a synthetic
                3D environment and integrating real-world trajectories derived
                from SinD, a bird's-eye-view dataset recorded from drone
                footage. The proposed dataset closes the gap between synthetic
                data and realistic trajectories, advancing the research in
                trajectory forecasting of road vehicles, offering both accurate
                multimodal ground-truth annotations and authentic human-driven
                vehicle trajectories.
              </p>
            </div>
            <image
              src="static/images/comparison_table.png"
              alt="Comparison Table Image"
              class="abstract-image"
            />
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->

    <section class="section">
      <div class="container is-max-desktop">
        <h2 class="title mb-5">Real Trajectories in a synthetic environment</h2>
        <div class="content has-text-justified">
          <div class="flex-container">
            <p>
              R3ST has been generated by rendering the virtual intersections
              created with Blender. Unlike typical synthetic datasets where
              vehicle motion is dictated by AI-driven or rule-based algorithms,
              R3ST incorporates real-world vehicle trajectories derived from two
              of the four scenarios proposed by
              <a
                class="link"
                href="https://github.com/SOTIF-AVLab/SinD/tree/main"
                >SinD</a
              >, a birdâ€™s-eye-view dataset with precise annotation of vehicle
              positions extracted from real drone footage.
            </p>
            <image
              src="static/images/trajectories.jpeg"
              alt="R3ST Trajectories"
              class="large-image"
            />
            <p class="italic">
              In the figure, a visualization of clustered vehicle trajectories
              in a R3ST crossroad. Each colored trajectory represents a cluster
              of similar paths, while the shaded regions indicate variance
              within each cluster.
            </p>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <h2 class="title">Multimodal Annotations</h2>
        <div class="content has-text-justified">
          <p>
            To enhance the usability of R3ST we leveraged
            <a href="https://github.com/Cartucho/vision_blender" class="link"
              >Vision Blender </a
            >to compute additional multimodal annotations that can be used in a
            range of computer-vision applications. Additionally, we directly
            derive the 3D bounding box of each object in the scene from the
            Blender World Environment and project them on the image plane to get
            the 2D bounding boxes.
          </p>
          <image
            src="static/images/multimodal_h.png"
            alt="R3ST Trajectories"
            class=""
          />
          <p class="italic">
            Qualitative results for instance segmentation (top two rows) and
            monocular depth estimation (bottom two rows). The first column shows
            RGB frames, the second column contains ground truth annotations,
            while the third and fourth columns present model predictions.
            Instance segmentation results are obtained using YOLO-Seg and SAM2
            online demo, while monocular depth estimation is performed with
            AnyDepth and Pixelformer Large pre-trained on KITTI.
          </p>
        </div>
      </div>
    </section>

    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{
          teglia2025rst,
          title={R3{ST}: A Synthetic Dataset with Real Trajectories for Urban Traffic Analysis},
          author={Simone Teglia and Claudia Melis Tonti and Francesco Pro and Leonardo Russo and Andrea Alfarano and Matteo Pentassuglia and Irene Amerini},
          booktitle={Synthetic Data for Computer Vision Workshop @ CVPR 2025},
          year={2025},
          url={https://openreview.net/forum?id=VCORXe6I5B}
          }</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the
                <a href="https://nerfies.github.io" target="_blank">Nerfies</a>
                project page. You are free to borrow the source code of this
                website, we just ask that you link back to this page in the
                footer. <br />
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>
  </body>
</html>
